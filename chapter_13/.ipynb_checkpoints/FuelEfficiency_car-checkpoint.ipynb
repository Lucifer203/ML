{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea109f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n",
    "               'Acceleration','Model Year','Origin']\n",
    "df = pd.read_csv(url,names=column_names,na_values='?',comment='\\t',\n",
    "                sep=\" \",skipinitialspace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f2ca7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 398 entries, 0 to 397\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   MPG           398 non-null    float64\n",
      " 1   Cylinders     398 non-null    int64  \n",
      " 2   Displacement  398 non-null    float64\n",
      " 3   Horsepower    392 non-null    float64\n",
      " 4   Weight        398 non-null    float64\n",
      " 5   Acceleration  398 non-null    float64\n",
      " 6   Model Year    398 non-null    int64  \n",
      " 7   Origin        398 non-null    int64  \n",
      "dtypes: float64(5), int64(3)\n",
      "memory usage: 25.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987157f",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85684cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb66bc19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement  Horsepower  Weight  Acceleration  \\\n",
       "0    18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1    15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2    18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3    16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4    17.0          8         302.0       140.0  3449.0          10.5   \n",
       "..    ...        ...           ...         ...     ...           ...   \n",
       "387  27.0          4         140.0        86.0  2790.0          15.6   \n",
       "388  44.0          4          97.0        52.0  2130.0          24.6   \n",
       "389  32.0          4         135.0        84.0  2295.0          11.6   \n",
       "390  28.0          4         120.0        79.0  2625.0          18.6   \n",
       "391  31.0          4         119.0        82.0  2720.0          19.4   \n",
       "\n",
       "     Model Year  Origin  \n",
       "0            70       1  \n",
       "1            70       1  \n",
       "2            70       1  \n",
       "3            70       1  \n",
       "4            70       1  \n",
       "..          ...     ...  \n",
       "387          82       1  \n",
       "388          82       2  \n",
       "389          82       1  \n",
       "390          82       1  \n",
       "391          82       1  \n",
       "\n",
       "[392 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd950365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "df_train,df_test =sklearn.model_selection.train_test_split(df,train_size=0.8,\n",
    "                                                          random_state=1)\n",
    "train_stats = df_train.describe().transpose()\n",
    "train_stats\n",
    "numeric_column_names = ['Cylinders','Displacement','Horsepower','Weight',\n",
    "                       'Acceleration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa36716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.901020</td>\n",
       "      <td>-0.736562</td>\n",
       "      <td>-0.950031</td>\n",
       "      <td>0.255202</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>19.4</td>\n",
       "      <td>0.351127</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>-0.340982</td>\n",
       "      <td>0.293190</td>\n",
       "      <td>0.548737</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.526556</td>\n",
       "      <td>1.144256</td>\n",
       "      <td>0.713897</td>\n",
       "      <td>1.339617</td>\n",
       "      <td>-0.625403</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>30.5</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.891280</td>\n",
       "      <td>-1.053025</td>\n",
       "      <td>-1.072585</td>\n",
       "      <td>0.475353</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.526556</td>\n",
       "      <td>1.563051</td>\n",
       "      <td>1.636916</td>\n",
       "      <td>1.470420</td>\n",
       "      <td>-1.359240</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement  Horsepower    Weight  Acceleration  \\\n",
       "203  28.0  -0.824303     -0.901020   -0.736562 -0.950031      0.255202   \n",
       "255  19.4   0.351127      0.413800   -0.340982  0.293190      0.548737   \n",
       "72   13.0   1.526556      1.144256    0.713897  1.339617     -0.625403   \n",
       "235  30.5  -0.824303     -0.891280   -1.053025 -1.072585      0.475353   \n",
       "37   14.0   1.526556      1.563051    1.636916  1.470420     -1.359240   \n",
       "\n",
       "     Model Year  Origin  \n",
       "203          76       3  \n",
       "255          78       1  \n",
       "72           72       1  \n",
       "235          77       1  \n",
       "37           71       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_norm,df_test_norm = df_train.copy(),df_test.copy()\n",
    "for col_name in numeric_column_names:\n",
    "    mean = train_stats.loc[col_name,'mean']\n",
    "    std = train_stats.loc[col_name,'std']\n",
    "    df_train_norm.loc[:,col_name]  = (df_train_norm.loc[:,col_name]-mean) / std\n",
    "    df_test_norm.loc[:,col_name] = (df_test_norm.loc[:,col_name]-mean) / std\n",
    "df_train_norm.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7006c",
   "metadata": {},
   "source": [
    "Bucketing the Model Year to simplify learning task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a5717e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7bc6c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = torch.tensor([73,76,79])\n",
    "v = torch.tensor(df_train_norm['Model Year'].values)\n",
    "df_train_norm['Model Year Bucketed'] = torch.bucketize(v,boundaries,\n",
    "                                                      right=True)\n",
    "v = torch.tensor(df_test_norm['Model Year'].values)\n",
    "df_test_norm['Model Year Bucketed'] = torch.bucketize(v,boundaries,\n",
    "                                                     right=True)\n",
    "numeric_column_names.append('Model Year Bucketed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "502f42f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>Cylinders</th>\n",
       "      <th>Displacement</th>\n",
       "      <th>Horsepower</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Acceleration</th>\n",
       "      <th>Model Year</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Model Year Bucketed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>27.2</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.530922</td>\n",
       "      <td>-0.499214</td>\n",
       "      <td>-0.555264</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>18.6</td>\n",
       "      <td>0.351127</td>\n",
       "      <td>0.345625</td>\n",
       "      <td>0.186457</td>\n",
       "      <td>0.776338</td>\n",
       "      <td>1.099115</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>29.0</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.891280</td>\n",
       "      <td>-0.525586</td>\n",
       "      <td>-0.874613</td>\n",
       "      <td>0.291894</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>37.2</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-1.008153</td>\n",
       "      <td>-1.000281</td>\n",
       "      <td>-1.110294</td>\n",
       "      <td>0.255202</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>33.0</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.823104</td>\n",
       "      <td>-0.762934</td>\n",
       "      <td>-0.908786</td>\n",
       "      <td>-0.552019</td>\n",
       "      <td>81</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>28.0</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.901020</td>\n",
       "      <td>-0.736562</td>\n",
       "      <td>-0.950031</td>\n",
       "      <td>0.255202</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>19.4</td>\n",
       "      <td>0.351127</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>-0.340982</td>\n",
       "      <td>0.293190</td>\n",
       "      <td>0.548737</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1.526556</td>\n",
       "      <td>1.144256</td>\n",
       "      <td>0.713897</td>\n",
       "      <td>1.339617</td>\n",
       "      <td>-0.625403</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>30.5</td>\n",
       "      <td>-0.824303</td>\n",
       "      <td>-0.891280</td>\n",
       "      <td>-1.053025</td>\n",
       "      <td>-1.072585</td>\n",
       "      <td>0.475353</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14.0</td>\n",
       "      <td>1.526556</td>\n",
       "      <td>1.563051</td>\n",
       "      <td>1.636916</td>\n",
       "      <td>1.470420</td>\n",
       "      <td>-1.359240</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MPG  Cylinders  Displacement  Horsepower    Weight  Acceleration  \\\n",
       "334  27.2  -0.824303     -0.530922   -0.499214 -0.555264     -0.001641   \n",
       "258  18.6   0.351127      0.345625    0.186457  0.776338      1.099115   \n",
       "139  29.0  -0.824303     -0.891280   -0.525586 -0.874613      0.291894   \n",
       "310  37.2  -0.824303     -1.008153   -1.000281 -1.110294      0.255202   \n",
       "349  33.0  -0.824303     -0.823104   -0.762934 -0.908786     -0.552019   \n",
       "..    ...        ...           ...         ...       ...           ...   \n",
       "203  28.0  -0.824303     -0.901020   -0.736562 -0.950031      0.255202   \n",
       "255  19.4   0.351127      0.413800   -0.340982  0.293190      0.548737   \n",
       "72   13.0   1.526556      1.144256    0.713897  1.339617     -0.625403   \n",
       "235  30.5  -0.824303     -0.891280   -1.053025 -1.072585      0.475353   \n",
       "37   14.0   1.526556      1.563051    1.636916  1.470420     -1.359240   \n",
       "\n",
       "     Model Year  Origin  Model Year Bucketed  \n",
       "334          81       1                    3  \n",
       "258          78       1                    2  \n",
       "139          74       2                    1  \n",
       "310          80       3                    3  \n",
       "349          81       2                    3  \n",
       "..          ...     ...                  ...  \n",
       "203          76       3                    2  \n",
       "255          78       1                    2  \n",
       "72           72       1                    0  \n",
       "235          77       1                    2  \n",
       "37           71       1                    0  \n",
       "\n",
       "[313 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2838e249",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m one_hot\n\u001b[0;32m----> 2\u001b[0m total_origin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(df_train_norm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrigin\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train_norm' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "total_origin = len(set(df_train_norm['Origin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "704375ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a5131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_encoded = one_hot(torch.from_numpy(\n",
    "    df_train_norm['Origin'].values)%total_origin)\n",
    "# origin_encoded\n",
    "x_train_numeric = torch.tensor(df_train_norm[numeric_column_names].values)\n",
    "x_train = torch.cat([x_train_numeric,origin_encoded],1).float()\n",
    "\n",
    "origin_encoded = one_hot(torch.from_numpy(\n",
    "    df_test_norm[\"Origin\"].values)%total_origin)\n",
    "x_test_numeric = torch.tensor(df_test_norm[numeric_column_names].values)\n",
    "x_test = torch.cat([x_test_numeric,origin_encoded],1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0443e511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8243, -0.5309, -0.4992,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [ 0.3511,  0.3456,  0.1865,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [-0.8243, -0.8913, -0.5256,  ...,  0.0000,  0.0000,  1.0000],\n",
       "        ...,\n",
       "        [ 1.5266,  1.1443,  0.7139,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [-0.8243, -0.8913, -1.0530,  ...,  0.0000,  1.0000,  0.0000],\n",
       "        [ 1.5266,  1.5631,  1.6369,  ...,  0.0000,  1.0000,  0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72dabe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(df_train_norm['MPG'].values).float()\n",
    "y_test = torch.tensor(df_test_norm['MPG'].values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4269c1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([27.2000, 18.6000, 29.0000, 37.2000, 33.0000, 13.0000, 22.4000, 44.6000,\n",
       "        27.4000, 31.0000, 24.0000, 28.0000, 17.6000, 15.0000, 11.0000, 12.0000,\n",
       "        26.8000, 13.0000, 21.6000, 20.8000, 16.9000, 20.0000, 26.5000, 18.0000,\n",
       "        26.0000, 24.0000, 29.5000, 20.2000, 32.8000, 14.0000, 16.0000, 31.8000,\n",
       "        30.7000, 34.1000, 20.0000, 15.0000, 28.4000, 20.0000, 13.0000, 35.0000,\n",
       "        31.0000, 44.0000, 27.0000, 17.0000, 34.1000, 14.0000, 23.0000, 28.1000,\n",
       "        14.5000, 24.0000, 29.9000, 36.0000, 23.9000, 15.0000, 16.0000, 20.0000,\n",
       "        16.0000, 24.0000, 23.9000, 11.0000, 14.0000, 19.0000, 30.0000, 21.0000,\n",
       "        15.0000, 23.0000, 18.0000, 29.0000, 18.0000, 12.0000, 33.0000, 29.5000,\n",
       "        19.0000, 14.0000, 19.0000, 26.6000, 36.0000, 38.0000, 24.0000, 13.0000,\n",
       "        16.0000, 17.5000, 19.0000, 16.0000, 27.0000, 16.5000, 20.3000, 24.0000,\n",
       "        30.0000, 23.0000, 17.0000, 18.0000, 25.5000, 13.0000, 24.0000, 16.0000,\n",
       "        26.0000, 19.4000, 17.0000, 30.9000, 18.0000, 23.7000, 19.0000, 30.5000,\n",
       "        21.0000, 39.1000, 24.0000, 22.0000, 36.0000, 13.0000, 14.0000, 28.0000,\n",
       "        18.0000, 32.3000, 44.3000, 19.1000,  9.0000, 25.0000, 26.0000, 20.6000,\n",
       "        19.0000, 21.1000, 23.0000, 36.4000, 32.0000, 16.0000, 17.7000, 19.0000,\n",
       "        35.1000, 15.0000, 14.0000, 26.0000, 19.0000, 15.5000, 11.0000, 39.4000,\n",
       "        32.7000, 16.2000, 36.0000, 32.0000, 34.0000, 22.0000, 13.0000, 23.0000,\n",
       "        43.1000, 18.0000, 26.0000, 22.0000, 17.5000, 34.2000, 14.0000, 24.0000,\n",
       "        26.0000, 13.0000, 13.0000, 35.0000, 18.5000, 23.8000, 14.0000, 21.0000,\n",
       "        31.0000, 15.5000, 27.0000, 20.0000, 16.0000, 23.0000, 29.0000, 13.0000,\n",
       "        25.4000, 31.0000, 37.0000, 23.5000, 13.0000, 20.2000, 37.7000, 29.0000,\n",
       "        26.4000, 25.5000, 24.0000, 32.1000, 40.8000, 18.2000, 25.0000, 16.0000,\n",
       "        15.0000, 20.0000, 25.4000, 25.0000, 39.0000, 27.0000, 23.0000, 24.5000,\n",
       "        18.0000, 25.0000, 27.9000, 12.0000, 17.0000, 25.0000, 17.6000, 21.0000,\n",
       "        12.0000, 29.0000, 26.0000, 26.0000, 32.0000, 15.0000, 46.6000, 15.0000,\n",
       "        23.2000, 18.0000, 16.5000, 21.5000, 25.0000, 34.5000, 18.0000, 31.6000,\n",
       "        25.0000, 34.4000, 18.0000, 17.5000, 21.0000, 17.5000, 27.0000, 31.0000,\n",
       "        34.7000, 18.0000, 31.0000, 18.1000, 25.1000, 18.1000, 26.0000, 18.0000,\n",
       "        34.0000, 37.3000, 22.0000, 14.0000, 36.1000, 25.0000, 14.0000, 22.0000,\n",
       "        32.0000, 33.8000, 27.2000, 15.5000, 14.0000, 23.0000, 27.0000, 33.7000,\n",
       "        15.0000, 19.8000, 15.0000, 29.0000, 27.5000, 27.0000, 13.0000, 15.0000,\n",
       "        20.6000, 18.5000, 10.0000, 33.0000, 14.0000, 22.0000, 27.2000, 22.0000,\n",
       "        25.8000, 38.1000, 10.0000, 22.0000, 13.0000, 22.3000, 16.0000, 28.0000,\n",
       "        28.0000, 26.0000, 28.0000, 24.0000, 16.0000, 27.0000, 36.0000, 30.0000,\n",
       "        16.5000, 29.8000, 15.0000, 24.3000, 25.0000, 31.3000, 32.2000, 14.0000,\n",
       "        26.0000, 13.0000, 32.9000, 21.5000, 30.0000, 12.0000, 30.0000, 16.0000,\n",
       "        20.2000, 24.2000, 20.5000, 31.5000, 25.0000, 20.2000, 30.0000, 15.0000,\n",
       "        32.0000, 28.0000, 26.6000, 18.0000, 28.0000, 19.4000, 13.0000, 30.5000,\n",
       "        14.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8b6a7",
   "metadata": {},
   "source": [
    "#### **Training a DNN regression model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac562f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "436c1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train,y_train)\n",
    "batch_size = 8\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(train_ds,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0531a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f6686f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=9, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [8,4]\n",
    "input_size = x_train.shape[1]\n",
    "all_layers = []\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size,hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size= hidden_unit\n",
    "all_layers.append(nn.Linear(hidden_units[-1],1))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5550b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf06a626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 6.1168\n",
      "Epoch 20 Loss 5.7736\n",
      "Epoch 40 Loss 5.7356\n",
      "Epoch 60 Loss 5.5430\n",
      "Epoch 80 Loss 5.3887\n",
      "Epoch 100 Loss 5.4836\n",
      "Epoch 120 Loss 5.0916\n",
      "Epoch 140 Loss 5.2251\n",
      "Epoch 160 Loss 5.2920\n",
      "Epoch 180 Loss 4.9798\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200\n",
    "log_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist_train = 0\n",
    "    for x_batch,y_batch in train_dl:\n",
    "        pred = model(x_batch)[:,0]\n",
    "        loss = loss_fn(pred,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        loss_hist_train += loss.item()\n",
    "    if epoch % log_epochs == 0:\n",
    "        print(f'Epoch {epoch} Loss 'f'{loss_hist_train/len(train_dl):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57b22bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 9.6504\n",
      "Test MAE: 1.9887\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pred = model(x_test.float())[:,0]\n",
    "    loss = loss_fn(pred,y_test)\n",
    "    print(f'Test MSE: {loss.item():.4f}')\n",
    "    print(f'Test MAE: {nn.L1Loss()(pred,y_test).item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49202781",
   "metadata": {},
   "source": [
    "# **Classifying MNIST handwritten digits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94720306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "image_path = './'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(root=image_path,train=True,\n",
    "                                                transform=transform,download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87736af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_dataset = torchvision.datasets.MNIST(root=image_path,\n",
    "                                               train=False,\n",
    "                                               transform=transform,\n",
    "                                               download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc47744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size=64\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(mnist_train_dataset,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e5cc837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = mnist_train_dataset[0][0].shape\n",
    "image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cbeaf78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [32,16]\n",
    "image_size = mnist_train_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size,hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "all_layers.append(nn.Linear(hidden_units[-1],10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eea43ea",
   "metadata": {},
   "source": [
    "Using the model for training,evaluation, and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4f54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Accuracy0.8531\n",
      "Epoch 1 Accuracy0.9287\n",
      "Epoch 2 Accuracy0.9413\n",
      "Epoch 3 Accuracy0.9506\n",
      "Epoch 4 Accuracy0.9558\n",
      "Epoch 5 Accuracy0.9592\n",
      "Epoch 6 Accuracy0.9627\n",
      "Epoch 7 Accuracy0.9650\n",
      "Epoch 8 Accuracy0.9674\n",
      "Epoch 9 Accuracy0.9690\n",
      "Epoch 10 Accuracy0.9710\n",
      "Epoch 11 Accuracy0.9729\n",
      "Epoch 12 Accuracy0.9739\n",
      "Epoch 13 Accuracy0.9750\n",
      "Epoch 14 Accuracy0.9764\n",
      "Epoch 15 Accuracy0.9777\n",
      "Epoch 16 Accuracy0.9779\n",
      "Epoch 17 Accuracy0.9798\n",
      "Epoch 18 Accuracy0.9807\n",
      "Epoch 19 Accuracy0.9813\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "torch.manual_seed(1)\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    accuracy_hist_train = 0\n",
    "    for x_batch,y_batch in train_dl:\n",
    "        pred=model(x_batch)\n",
    "        loss = loss_fn(pred,y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        is_correct = (torch.argmax(pred,dim=1)==y_batch).float()\n",
    "        accuracy_hist_train += is_correct.sum()\n",
    "    accuracy_hist_train /= len(train_dl.dataset)\n",
    "    print(f'Epoch {epoch} Accuracy'f'{accuracy_hist_train:.4f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd002ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9653\n"
     ]
    }
   ],
   "source": [
    "pred = model(mnist_test_dataset.data/255.)\n",
    "is_correct = (torch.argmax(pred,dim=1)==mnist_test_dataset.targets).float()\n",
    "print(f'Test accuracy: {is_correct.mean():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ebf20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.1.4-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.12.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2023.4.0)\n",
      "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
      "  Downloading torchmetrics-1.3.0.post0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from pytorch-lightning) (4.9.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
      "  Downloading lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: requests in /home/lucifer/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.8.5)\n",
      "Requirement already satisfied: setuptools in /home/lucifer/anaconda3/lib/python3.11/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (68.0.0)\n",
      "Requirement already satisfied: filelock in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (3.9.0)\n",
      "Requirement already satisfied: sympy in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->pytorch-lightning) (12.3.101)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.12.0->pytorch-lightning) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lucifer/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.12.0->pytorch-lightning) (1.3.0)\n",
      "Downloading pytorch_lightning-2.1.4-py3-none-any.whl (778 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Downloading torchmetrics-1.3.0.post0-py3-none-any.whl (840 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
      "Successfully installed lightning-utilities-0.10.1 pytorch-lightning-2.1.4 torchmetrics-1.3.0.post0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346c126",
   "metadata": {},
   "source": [
    "Setting up the Pytorch Lightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41982199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2b1da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "hidden_units = [32,16]\n",
    "\n",
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self,image_shape=(1,28,28),hidden_state=(32,16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        #PL attributes:\n",
    "#         self.train_acc = Accuracy(task=\"MULTILABEL\")\n",
    "#         self.valid_acc = Accuracy(task=\"MULTILABEL\")\n",
    "#         self.test_acc = Accuracy(task=\"MULTILABEL\")\n",
    "        self.train_acc = Accuracy(task=\"MULTILABEL\", num_labels=10)\n",
    "        self.valid_acc = Accuracy(task=\"MULTILABEL\", num_labels=10)\n",
    "        self.test_acc = Accuracy(task=\"MULTILABEL\", num_labels=10)\n",
    "\n",
    "        \n",
    "        #Model similar to previous section made using torch.nn\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units:\n",
    "            layer = nn.Linear(input_size,hidden_unit)\n",
    "            all_layers.append(layer)\n",
    "            all_layers.append(nn.ReLU())\n",
    "            input_size = hidden_unit\n",
    "        \n",
    "        all_layers.append(nn.Linear(hidden_units[-1],10))\n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(self(x),y)\n",
    "        preds = torch.argmax(logits,dim=1)\n",
    "        self.train_acc.update(preds,y)\n",
    "        self.log(\"train_loss\",loss,prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self,outs):\n",
    "        self.log(\"train_acc\",self.train_acc.compute())\n",
    "        \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(self(x),y)\n",
    "        preds = torch.argmax(logits,dim=1)\n",
    "        self.valid_acc.update(preds,y)\n",
    "        self.log(\"valid_loss\",loss,prog_bar=True)\n",
    "        self.log(\"valid_acc\",self.valid_acc.compute(),prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        logits = self(x)\n",
    "        loss =nn.functional.cross_entropy(self(x),y)\n",
    "        preds = torch.argmax(logits,dim=1)\n",
    "        self.test_acc.update(preds,y)\n",
    "        self.log(\"test_loss\",loss,prog_bar=True)\n",
    "        self.log(\"test_acc\",self.test_acc.compute(),prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a0ec45",
   "metadata": {},
   "source": [
    "Setting up the data loaders for Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b93c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8025f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self,data_path=\"./\"):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path,download=True)\n",
    "        \n",
    "    def setup(self,stage=None):\n",
    "        #stage is either fit validate test or predict\n",
    "        mnist_all = MNIST(root=self.data_path,\n",
    "                         train=True,\n",
    "                         transform=self.transform,\n",
    "                         download=False)\n",
    "        \n",
    "        self.train,self.val = random_split(mnist_all,\n",
    "                                          [55000,5000],generator=torch.Generator().manual_seed(1))\n",
    "        self.test = MNIST(root = self.data_path,\n",
    "                         train=False,\n",
    "                         transform=self.transform,\n",
    "                         download=False)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train,batch_size=64,num_workers=4)\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(self.val,batch_size=64,num_workers=4)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.train,batch_size=64,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6ad76738",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "mnist_dm = MnistDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c66fc894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MnistDataModule at 0x7fc6cc5753d0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157cdb0",
   "metadata": {},
   "source": [
    "Training the model using PyTorch Lightning Trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee579851",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | train_acc | MultilabelAccuracy | 0     \n",
      "1 | valid_acc | MultilabelAccuracy | 0     \n",
      "2 | test_acc  | MultilabelAccuracy | 0     \n",
      "3 | model     | Sequential         | 25.8 K\n",
      "-------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc835c3ecd774a169ec9880c8d75bf5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                     | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model\u001b[38;5;241m=\u001b[39mmnistclassifier,datamodule\u001b[38;5;241m=\u001b[39mmnist_dm)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\n\u001b[1;32m    266\u001b[0m     trainer,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    268\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch,\n\u001b[1;32m    269\u001b[0m     batch_idx,\n\u001b[1;32m    270\u001b[0m     optimizer,\n\u001b[1;32m    271\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1291\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1254\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1257\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1291\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39moptimizer_closure)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[39], line 39\u001b[0m, in \u001b[0;36mMultiLayerPerceptron.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28mself\u001b[39m(x),y)\n\u001b[1;32m     38\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_acc\u001b[38;5;241m.\u001b[39mupdate(preds,y)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,loss,prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/metric.py:465\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/classification/stat_scores.py:476\u001b[0m, in \u001b[0;36mMultilabelStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 476\u001b[0m     _multilabel_stat_scores_tensor_validation(\n\u001b[1;32m    477\u001b[0m         preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multilabel_stat_scores_format(\n\u001b[1;32m    480\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    481\u001b[0m )\n\u001b[1;32m    482\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multilabel_stat_scores_update(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchmetrics/functional/classification/stat_scores.py:616\u001b[0m, in \u001b[0;36m_multilabel_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_labels, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# Check that they have same shape\u001b[39;00m\n\u001b[1;32m    614\u001b[0m _check_same_shape(preds, target)\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m num_labels:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected both `target.shape[1]` and `preds.shape[1]` to be equal to the number of labels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    619\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    620\u001b[0m     )\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# Check that target only contains [0,1] values or value in ignore_index\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(max_epochs=10,gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(model=mnistclassifier,datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b77fbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
